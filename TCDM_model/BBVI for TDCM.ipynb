{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TDCModel"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra, Distributions, Combinatorics, Random, Kronecker, SpecialFunctions\n",
    "using ResumableFunctions\n",
    "include(\"../DCM_model/BBVI_utils.jl\")\n",
    "\n",
    "# Object for observed data\n",
    "struct TDCMObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    Y           :: Array{Int, 3}\n",
    "    Q           :: Matrix{Int}\n",
    "    D           :: Vector{Matrix{Int}}\n",
    "    U           :: Vector{Vector{Matrix{T}}}\n",
    "    X           :: Vector{Vector{Matrix{T}}}\n",
    "    group       :: Vector{Int}\n",
    "    skill_dict  :: Dict{Int, Vector{Int}}\n",
    "end\n",
    "\n",
    "function TDCMObs(\n",
    "    Y       :: Array{Int, 3}, \n",
    "    Q       :: Matrix{Int},\n",
    "    U       :: Vector{Vector{Matrix{T}}},\n",
    "    X       :: Vector{Vector{Matrix{T}}},\n",
    "    group   :: Vector{Int}) where T <: AbstractFloat\n",
    "    D = generate_delta(Q)\n",
    "    K, L = size(Q, 2), size(D[1], 1)\n",
    "    skill_dict = Dict{Int, Vector{Int}}()\n",
    "    for l in 0:(L - 1)\n",
    "        skill_dict[l + 1] = reverse(digits(l, base=2, pad=K))\n",
    "    end\n",
    "    TDCMObs(Y, Q, D, U, X, group, skill_dict)\n",
    "end\n",
    "\n",
    "# Object including latent variables and model parameters\n",
    "struct TDCModel{T <: AbstractFloat}\n",
    "    # data\n",
    "    obs         :: TDCMObs\n",
    "    # prior distribution parameters\n",
    "    mu_beta_prior       :: Vector{Vector{T}}\n",
    "    V_beta_prior        :: Vector{Matrix{T}}\n",
    "    mu_omega_prior      :: Vector{Vector{Vector{Vector{Vector{T}}}}}\n",
    "    V_omega_prior       :: Vector{Vector{Vector{Vector{Matrix{T}}}}}\n",
    "    a_tau_prior         :: Vector{Vector{Vector{Vector{T}}}}\n",
    "    b_tau_prior         :: Vector{Vector{Vector{Vector{T}}}}\n",
    "    # This option allocates extra memory based on number of threads availible in the environment\n",
    "    enable_parallel     :: Bool\n",
    "    # Variational distribution parameters\n",
    "    pi_star             :: Vector{Vector{Vector{T}}}\n",
    "    mu_beta_star        :: Vector{Vector{T}}\n",
    "    V_beta_star         :: Vector{Matrix{T}}\n",
    "    mu_gamma_star       :: Vector{Vector{Vector{Vector{Vector{T}}}}}\n",
    "    V_gamma_star        :: Vector{Vector{Vector{Vector{Matrix{T}}}}}\n",
    "    mu_omega_star       :: Vector{Vector{Vector{Vector{Vector{T}}}}}\n",
    "    V_omega_star        :: Vector{Vector{Vector{Vector{Matrix{T}}}}}\n",
    "    a_tau_star          :: Vector{Vector{Vector{Vector{T}}}}\n",
    "    b_tau_star          :: Vector{Vector{Vector{Vector{T}}}}\n",
    "    # Number of samples for noisy gradient\n",
    "    M                   :: Int\n",
    "    # Preallocated storage for samples from variational distribution\n",
    "    Z_sample            :: Vector{Vector{Vector{Vector{Int}}}}\n",
    "    beta_sample         :: Vector{Vector{Vector{T}}}\n",
    "    gamma_sample        :: Vector{Vector{Vector{Vector{Vector{Vector{T}}}}}}\n",
    "    omega_sample        :: Vector{Vector{Vector{Vector{Vector{Vector{T}}}}}}\n",
    "    tau_sample          :: Vector{Vector{Vector{Vector{Vector{T}}}}}\n",
    "    # Preallocated storage for noisy gradient descent calculations\n",
    "    storage_L           :: Vector{T}\n",
    "    storage_L2          :: Vector{T}\n",
    "    storage_L3          :: Vector{T}\n",
    "    storage_L4          :: Vector{T}\n",
    "    storage_LL          :: Matrix{T}\n",
    "    storage_LL2         :: Matrix{T}\n",
    "    storage_LL3         :: Matrix{T}\n",
    "    storage_LL4         :: Matrix{T}\n",
    "    # Preallocated storage for matrix vectorization operations\n",
    "    storage_comm        :: Matrix{T}\n",
    "    storage_dup         :: Matrix{T}\n",
    "    storage_Lsqr        :: Vector{T}\n",
    "    storage_Lsqr2       :: Vector{T}\n",
    "    storage_L2L2        :: Matrix{T}\n",
    "    storage_C           :: Matrix{T}\n",
    "    storage_gradC       :: Vector{T}\n",
    "    # Preallocated Identity matrix\n",
    "    I_LL                :: Matrix{T}\n",
    "    # Preallocated storage for parallel noisy gradient descent calculations\n",
    "    storage_L_par       :: Vector{Vector{T}}\n",
    "    storage_L2_par      :: Vector{Vector{T}}\n",
    "    storage_L3_par      :: Vector{Vector{T}}\n",
    "    storage_L4_par      :: Vector{Vector{T}}\n",
    "    storage_LL_par      :: Vector{Matrix{T}}\n",
    "    storage_LL2_par     :: Vector{Matrix{T}}\n",
    "    storage_LL3_par     :: Vector{Matrix{T}}\n",
    "    storage_LL4_par     :: Vector{Matrix{T}}\n",
    "    # Preallocated storage for parallel matrix vectorization operations\n",
    "    storage_comm_par    :: Vector{Matrix{T}}\n",
    "    storage_dup_par     :: Vector{Matrix{T}}\n",
    "    storage_Lsqr_par    :: Vector{Vector{T}}\n",
    "    storage_Lsqr2_par   :: Vector{Vector{T}}\n",
    "    storage_L2L2_par    :: Vector{Matrix{T}}\n",
    "    storage_C_par       :: Vector{Matrix{T}}\n",
    "    storage_gradC_par   :: Vector{Vector{T}}\n",
    "end\n",
    "\n",
    "function TDCModel(\n",
    "    obs                 :: TDCMObs,\n",
    "    mu_beta_prior       :: Vector{Vector{T}},\n",
    "    V_beta_prior        :: Vector{Matrix{T}},\n",
    "    mu_omega_prior      :: Vector{Vector{Vector{Vector{Vector{T}}}}},\n",
    "    V_omega_prior       :: Vector{Vector{Vector{Vector{Matrix{T}}}}},\n",
    "    a_tau_prior         :: Vector{Vector{Vector{Vector{T}}}},\n",
    "    b_tau_prior         :: Vector{Vector{Vector{Vector{T}}}},\n",
    "    M                   :: Int;\n",
    "    # This option allocates extra memory based on number of threads availible in the environment\n",
    "    enable_parallel     :: Bool=false\n",
    ") where T <: AbstractFloat\n",
    "    # Number of students, time points, questions, skills, attribute profiles, groups\n",
    "    N, O, J, K, L, S = size(obs.Y, 1), size(obs.Y, 2), size(obs.Y, 3),  size(obs.Q, 2), size(obs.D[1], 1), size(obs.U[1][1], 1)\n",
    "    # Initialize variational distribution parameters\n",
    "    pi_star = Vector{Vector{Vector{T}}}(undef, N)\n",
    "    for i in 1:N\n",
    "        pi_star[i] = Vector{Vector{T}}(undef, O)\n",
    "        for t in 1:O\n",
    "            # probability vector of possible single skill mastery over all time points\n",
    "            pi_star[i][t] = ones(2^K) ./ 2^K\n",
    "        end\n",
    "    end\n",
    "    mu_beta_star = Vector{Vector{T}}(undef, J)\n",
    "    V_beta_star = Vector{Matrix{T}}(undef, J)\n",
    "    for j in 1:J\n",
    "        num_features = size(obs.D[j], 2)\n",
    "        mu_beta_star[j] = zeros(num_features)\n",
    "        V_beta_star[j] = Matrix(1.0I, num_features, num_features)\n",
    "    end\n",
    "    mu_gamma_star = Vector{Vector{Vector{Vector{Vector{T}}}}}(undef, K)\n",
    "    V_gamma_star = Vector{Vector{Vector{Vector{Matrix{T}}}}}(undef, K)\n",
    "    for k in 1:K\n",
    "        mu_gamma_star[k] = Vector{Vector{Vector{Vector{T}}}}(undef, O)\n",
    "        V_gamma_star[k] = Vector{Vector{Vector{Matrix{T}}}}(undef, O)\n",
    "        for t in 1:O\n",
    "            if t == 1\n",
    "                mu_gamma_star[k][t] = Vector{Vector{Vector{T}}}(undef, 1)\n",
    "                V_gamma_star[k][t] = Vector{Vector{Matrix{T}}}(undef, 1)\n",
    "\n",
    "                mu_gamma_star[k][t][1] = Vector{Vector{T}}(undef, S)\n",
    "                V_gamma_star[k][t][1] = Vector{Matrix{T}}(undef, S)\n",
    "                for s in 1:S\n",
    "                    mu_gamma_star[k][t][1][s] = zeros(1)\n",
    "                    V_gamma_star[k][t][1][s] = ones(1, 1)\n",
    "                end\n",
    "            else\n",
    "                mu_gamma_star[k][t] = Vector{Vector{Vector{T}}}(undef, 2)\n",
    "                V_gamma_star[k][t] = Vector{Vector{Matrix{T}}}(undef, 2)\n",
    "                num_features = size(obs.X[k][t], 2)\n",
    "                for z in 1:2\n",
    "                    mu_gamma_star[k][t][z] = Vector{Vector{T}}(undef, S)\n",
    "                    V_gamma_star[k][t][z] = Vector{Matrix{T}}(undef, S)\n",
    "                    for s in 1:S\n",
    "                        mu_gamma_star[k][t][z][s] = zeros(num_features)\n",
    "                        V_gamma_star[k][t][z][s] = Matrix(1.0I, num_features, num_features)\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    mu_omega_star = Vector{Vector{Vector{Vector{Vector{T}}}}}(undef, K)\n",
    "    V_omega_star = Vector{Vector{Vector{Vector{Matrix{T}}}}}(undef, K)\n",
    "    a_tau_star = Vector{Vector{Vector{Vector{T}}}}(undef, K)\n",
    "    b_tau_star = Vector{Vector{Vector{Vector{T}}}}(undef, K)\n",
    "    for k in 1:K\n",
    "        mu_omega_star[k] = Vector{Vector{Vector{Vector{T}}}}(undef, O)\n",
    "        V_omega_star[k] = Vector{Vector{Vector{Matrix{T}}}}(undef, O)\n",
    "        a_tau_star[k] = Vector{Vector{Vector{T}}}(undef, O)\n",
    "        b_tau_star[k] = Vector{Vector{Vector{T}}}(undef, O)\n",
    "        for t in 1:O\n",
    "            num_features_gamma = size(obs.X[k][t], 2)\n",
    "            num_features_omega = size(obs.U[k][t], 2)\n",
    "            if t == 1\n",
    "                mu_omega_star[k][t] = Vector{Vector{Vector{T}}}(undef, 1)\n",
    "                mu_omega_star[k][t][1] = Vector{Vector{T}}(undef, 1)\n",
    "                mu_omega_star[k][t][1][1] = zeros(num_features_omega)\n",
    "\n",
    "                V_omega_star[k][t] = Vector{Vector{Matrix{T}}}(undef, 1)\n",
    "                V_omega_star[k][t][1] = Vector{Matrix{T}}(undef, 1)\n",
    "                V_omega_star[k][t][1][1] = Matrix{T}(1.0I, num_features_omega, num_features_omega)\n",
    "\n",
    "                a_tau_star[k][t] = Vector{Vector{T}}(undef, 1)\n",
    "                a_tau_star[k][t][1] = ones(1)\n",
    "\n",
    "                b_tau_star[k][t] = Vector{Vector{T}}(undef, 1)\n",
    "                b_tau_star[k][t][1] = ones(1)\n",
    "            else\n",
    "                mu_omega_star[k][t] = Vector{Vector{Vector{T}}}(undef, 2)\n",
    "                V_omega_star[k][t] = Vector{Vector{Matrix{T}}}(undef, 2)\n",
    "                a_tau_star[k][t] = Vector{Vector{T}}(undef, 2)\n",
    "                a_tau_star[k][t] = Vector{Vector{T}}(undef, 2)\n",
    "                for z in 1:2\n",
    "                    mu_omega_star[k][t][z] = Vector{Vector{T}}(undef, num_features_gamma)\n",
    "                    V_omega_star[k][t][z] = Vector{Matrix{T}}(undef, num_features_gamma)\n",
    "                    a_tau_star[k][t][z] = ones(num_features_gamma) .* 3\n",
    "                    a_tau_star[k][t][z] = ones(num_features_gamma) .* 3\n",
    "                    for m in 1:num_features_gamma\n",
    "                        mu_omega_star[k][t][z][m] = zeros(num_features_omega)\n",
    "                        V_omega_star[k][t][z][m] = Matrix(1.0I, num_features_omega, num_features_omega)\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # Preallocate space for samples from variational distribution\n",
    "    Z_sample = Vector{Vector{Vector{Vector{Int}}}}(undef, N)\n",
    "    for i in 1:N\n",
    "        Z_sample[i] = Vector{Vector{Vector{Int}}}(undef, O)\n",
    "        for t in 1:O\n",
    "            Z_sample[i][t] = Vector{Vector{Int}}(undef, M)\n",
    "            for m in 1:M\n",
    "                Z_sample[i][t][m] = Vector{Int}(undef, 2^(K))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    beta_sample = Vector{Vector{Vector{T}}}(undef, J)\n",
    "    for j in 1:J\n",
    "        beta_sample[j] = Vector{Vector{T}}(undef, M)\n",
    "        num_features = size(obs.D[j], 2)\n",
    "        for m in 1:M\n",
    "            beta_sample[j][m] = Vector{T}(undef, num_features)\n",
    "        end\n",
    "    end\n",
    "    gamma_sample = Vector{Vector{Vector{Vector{Vector{Vector{T}}}}}}(undef, K)\n",
    "    for k in 1:K\n",
    "        gamma_sample[k] = Vector{Vector{Vector{Vector{Vector{T}}}}}(undef, O)\n",
    "        for t in 1:O\n",
    "            if t == 1\n",
    "                gamma_sample[k][t] = Vector{Vector{Vector{Vector{T}}}}(undef, 1)\n",
    "                gamma_sample[k][t][1] = Vector{Vector{Vector{T}}}(undef, S)\n",
    "                for s in 1:S\n",
    "                    gamma_sample[k][t][1][s] = Vector{Vector{T}}(undef, M)\n",
    "                    for m in 1:M\n",
    "                        gamma_sample[k][t][1][s][m] = Vector{T}(undef, 1)\n",
    "                    end\n",
    "                end\n",
    "            else\n",
    "                num_features = size(obs.X[k][t], 2)\n",
    "                gamma_sample[k][t] = Vector{Vector{Vector{Vector{T}}}}(undef, 2)\n",
    "                for z in 1:2\n",
    "                    gamma_sample[k][t][z] = Vector{Vector{Vector{T}}}(undef, S)\n",
    "                    for s in 1:S\n",
    "                        gamma_sample[k][t][z][s] = Vector{Vector{T}}(undef, M)\n",
    "                        for m in 1:M\n",
    "                            gamma_sample[k][t][z][s][m] = Vector{T}(undef, num_features)\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    omega_sample = Vector{Vector{Vector{Vector{Vector{Vector{T}}}}}}(undef, K)\n",
    "    tau_sample = Vector{Vector{Vector{Vector{Vector{T}}}}}(undef, K)\n",
    "    for k in 1:K\n",
    "        omega_sample[k] = Vector{Vector{Vector{Vector{Vector{T}}}}}(undef, O)\n",
    "        tau_sample[k] = Vector{Vector{Vector{Vector{T}}}}(undef, O)\n",
    "        for t in 1:O\n",
    "            num_features_gamma = size(obs.X[k][t], 2)\n",
    "            num_features_omega = size(obs.U[k][t], 2)\n",
    "            if t == 1\n",
    "                omega_sample[k][t] = Vector{Vector{Vector{Vector{T}}}}(undef, 1)\n",
    "                omega_sample[k][t][1] = Vector{Vector{Vector{T}}}(undef, 1)\n",
    "                omega_sample[k][t][1][1] = Vector{Vector{T}}(undef, M)\n",
    "\n",
    "                tau_sample[k][t] = Vector{Vector{Vector{T}}}(undef, 1)\n",
    "                tau_sample[k][t][1] = Vector{Vector{T}}(undef, 1)\n",
    "                tau_sample[k][t][1][1] = Vector{T}(undef, M)\n",
    "                for m in 1:M\n",
    "                    omega_sample[k][t][1][1][m] = Vector{T}(undef, num_features_omega)\n",
    "                end\n",
    "            else\n",
    "                omega_sample[k][t] = Vector{Vector{Vector{Vector{T}}}}(undef, 2)\n",
    "                tau_sample[k][t] = Vector{Vector{Vector{T}}}(undef, 2)\n",
    "                for z in 1:2\n",
    "                    omega_sample[k][t][z] = Vector{Vector{Vector{T}}}(undef, num_features_gamma)\n",
    "                    tau_sample[k][t][z] = Vector{Vector{T}}(undef, num_features_gamma)\n",
    "                    for g in 1:num_features_gamma\n",
    "                        omega_sample[k][t][z][g] = Vector{Vector{T}}(undef, M)\n",
    "                        tau_sample[k][t][z][g] = Vector{T}(undef, M)\n",
    "                        for m in 1:M\n",
    "                            omega_sample[k][t][z][g][m] = Vector{T}(undef, num_features_omega)\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # Preallocate storage for noisy gradient descent calculations\n",
    "    storage_L = Vector{T}(undef, L)\n",
    "    storage_L2 = similar(storage_L)\n",
    "    storage_L3 = similar(storage_L)\n",
    "    storage_L4 = similar(storage_L)\n",
    "    storage_LL = Matrix{T}(undef, L, L)\n",
    "    storage_LL2 = similar(storage_LL)\n",
    "    storage_LL3 = similar(storage_LL)\n",
    "    storage_LL4 = similar(storage_LL)\n",
    "    # Preallocate storage for matrix vectorization operations\n",
    "    storage_comm = Matrix{T}(undef, L^2, L^2)\n",
    "    storage_dup = Matrix{T}(undef, L^2, Int(L*(L+1)/2))\n",
    "    storage_Lsqr = Vector{T}(undef, L^2)\n",
    "    storage_Lsqr2 = Vector{T}(undef, L^2)\n",
    "    storage_L2L2 = Matrix{T}(undef, L^2, L^2)\n",
    "    storage_C = Matrix{T}(undef, L, L)\n",
    "    storage_gradC = Vector{T}(undef, Int(L*(L+1)/2))\n",
    "    # Preallocate Identity matrix\n",
    "    I_LL = Matrix{T}(I, L, L)\n",
    "    # Allocate optional space for parallel computing\n",
    "    nthreads = Threads.nthreads()\n",
    "    storage_L_par = Vector{Vector{T}}(undef, nthreads)\n",
    "    storage_L2_par = similar(storage_L_par)\n",
    "    storage_L3_par = similar(storage_L_par)\n",
    "    storage_L4_par = similar(storage_L_par)\n",
    "    storage_LL_par = Vector{Matrix{T}}(undef, nthreads)\n",
    "    storage_LL2_par = similar(storage_LL_par)\n",
    "    storage_LL3_par = similar(storage_LL_par)\n",
    "    storage_LL4_par = similar(storage_LL_par)\n",
    "    storage_comm_par = Vector{Matrix{T}}(undef, nthreads)\n",
    "    storage_dup_par = Vector{Matrix{T}}(undef, nthreads)\n",
    "    storage_Lsqr_par = Vector{Vector{T}}(undef, nthreads)\n",
    "    storage_Lsqr2_par = Vector{Vector{T}}(undef, nthreads)\n",
    "    storage_L2L2_par = Vector{Matrix{T}}(undef, nthreads)\n",
    "    storage_C_par = Vector{Matrix{T}}(undef, nthreads)\n",
    "    storage_gradC_par = Vector{Vector{T}}(undef, nthreads)\n",
    "    if enable_parallel\n",
    "        storage_L_par[1] = storage_L\n",
    "        storage_L2_par[1] = storage_L2\n",
    "        storage_L3_par[1] = storage_L3\n",
    "        storage_L4_par[1] = storage_L4\n",
    "        storage_LL_par[1] = storage_LL\n",
    "        storage_LL2_par[1] = storage_LL2\n",
    "        storage_LL3_par[1] = storage_LL3\n",
    "        storage_LL4_par[1] = storage_LL4\n",
    "        storage_comm_par[1] = storage_comm\n",
    "        storage_dup_par[1] = storage_dup\n",
    "        storage_Lsqr_par[1] = storage_Lsqr\n",
    "        storage_Lsqr2_par[1] = storage_Lsqr2\n",
    "        storage_L2L2_par[1] = storage_L2L2\n",
    "        storage_C_par[1] = storage_C\n",
    "        storage_gradC_par[1] = storage_gradC_par\n",
    "        for thread in 2:nthreads\n",
    "            storage_L_par[thread] = Vector{T}(undef, L)\n",
    "            storage_L2_par[thread] = similar(storage_L)\n",
    "            storage_L3_par[thread] = similar(storage_L)\n",
    "            storage_L4_par[thread] = similar(storage_L)\n",
    "            storage_LL_par[thread] = Matrix{T}(undef, L, L)\n",
    "            storage_LL2_par[thread] = similar(storage_LL)\n",
    "            storage_LL3_par[thread] = similar(storage_LL)\n",
    "            storage_LL4_par[thread] = similar(storage_LL)\n",
    "            storage_comm_par[thread] = Matrix{T}(undef, L^2, L^2)\n",
    "            storage_dup_par[thread] = Matrix{T}(undef, L^2, Int(L*(L+1)/2))\n",
    "            storage_Lsqr_par[thread] = Vector{T}(undef, L^2)\n",
    "            storage_Lsqr2_par[thread] = Vector{T}(undef, L^2)\n",
    "            storage_L2L2_par[thread] = Matrix{T}(undef, L^2, L^2)\n",
    "            storage_C_par[thread] = Matrix{T}(undef, L, L)\n",
    "            storage_gradC_par[thread] = Vector{T}(undef, Int(L*(L+1)/2))\n",
    "        end\n",
    "        println(\"TDCModel constructed for computation on $nthreads threads\")\n",
    "    end\n",
    "    # Initialize DCModel object\n",
    "    TDCModel(obs, mu_beta_prior, V_beta_prior, mu_omega_prior, V_omega_prior, a_tau_prior, b_tau_prior, enable_parallel,\n",
    "    pi_star, mu_beta_star, V_beta_star, mu_gamma_star, V_gamma_star, mu_omega_star, V_omega_star, a_tau_star, b_tau_star, M,\n",
    "    Z_sample, beta_sample, gamma_sample, omega_sample, tau_sample,\n",
    "    storage_L, storage_L2, storage_L3, storage_L4, storage_LL, storage_LL2, storage_LL3, storage_LL4,\n",
    "    storage_comm, storage_dup, storage_Lsqr, storage_Lsqr2, storage_L2L2, storage_C, storage_gradC,\n",
    "    I_LL, storage_L_par, storage_L2_par, storage_L3_par, storage_L4_par, storage_LL_par, storage_LL2_par, storage_LL3_par, storage_LL4_par,\n",
    "    storage_comm_par, storage_dup_par, storage_Lsqr_par, storage_Lsqr2_par, storage_L2L2_par, storage_C_par, storage_gradC_par)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_Z(\n",
    "    model           :: TDCModel,\n",
    "    idx_student     :: Int,\n",
    "    idx_time        :: Int\n",
    ")\n",
    "    # Create variational distribution from model parameters for specific Z\n",
    "    Z_it_variational_distribution = Multinomial(1, model.pi_star[idx_student][idx_time])\n",
    "    # Populate preallocated arrays with samples from variational distribution\n",
    "    rand!(Z_it_variational_distribution, model.Z_sample[idx_student][idx_time])\n",
    "end\n",
    "\n",
    "function sample_β(\n",
    "    model           :: TDCModel;\n",
    "    idx_question    :: Int = -1\n",
    ")\n",
    "    obs = model.obs\n",
    "    J = size(obs.Y, 3)\n",
    "    if idx_question == -1\n",
    "        for j in 1:J\n",
    "            # Create variational distribution from model parameters for each β_j\n",
    "            beta_j_variational_distribution = MvNormal(model.mu_beta_star[j], model.V_beta_star[j])\n",
    "            # Populate preallocated arrays with samples from variational distribution\n",
    "            rand!(beta_j_variational_distribution, model.beta_sample[j])\n",
    "        end\n",
    "    else\n",
    "        # Create variational distribution from model parameters for specific β_j\n",
    "        beta_j_variational_distribution = MvNormal(model.mu_beta_star[idx_question], model.V_beta_star[idx_question])\n",
    "        # Populate preallocated arrays with samples from variational distribution\n",
    "        rand!(beta_j_variational_distribution, model.beta_sample[idx_question])\n",
    "    end\n",
    "end\n",
    "\n",
    "function sample_γ(\n",
    "    model           :: TDCModel,\n",
    "    idx_group       :: Int,\n",
    "    idx_time        :: Int,\n",
    "    idx_skill       :: Int,\n",
    "    indicator_skill :: Int\n",
    ")\n",
    "    # Create variational distribution from model parameters for γ\n",
    "    gamma_stkz_variational_distribution = MvNormal(model.mu_gamma_star[idx_skill][idx_time][indicator_skill + 1][idx_group],\n",
    "                                            model.V_gamma_star[idx_skill][idx_time][indicator_skill + 1][idx_group])\n",
    "    # Populate preallocated arrays with samples from variational distribution\n",
    "    rand!(gamma_stkz_variational_distribution, model.gamma_sample[idx_skill][idx_time][indicator_skill + 1][idx_group])\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_categorical_variational_distribution (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function update_categorical_variational_distribution(\n",
    "#     params              :: Vector{T},\n",
    "#     modelloglikelihood_func, \n",
    "#     storage_samples     :: Vector{Vector{Int}};\n",
    "#     step                :: T = 1e-2,\n",
    "#     maxiter             :: Int = 100000,\n",
    "#     verbose             :: Bool = true,\n",
    "#     storage1            :: Vector{T} = nothing,\n",
    "#     storage2            :: Vector{T} = nothing,\n",
    "#     storage3            :: Vector{T} = nothing,\n",
    "#     storage4            :: Vector{T} = nothing\n",
    "# ) where T <: AbstractFloat\n",
    "#     # Storage for gradient terms\n",
    "#     grad_log_q = storage1\n",
    "#     grad_L = model.storage2\n",
    "#     if storage_grad1 == nothing\n",
    "#         grad_log_q = similar(params)\n",
    "#     end\n",
    "#     if storage_grad2 == nothing\n",
    "#         grad_L = similar(params)\n",
    "#     end\n",
    "#     # Storage for intermediate term in gradient calculations\n",
    "#     D_beta = storage3\n",
    "#     rho_star_old_i = storage4\n",
    "#     if storage3 == nothing\n",
    "#         D_beta = similar(params)\n",
    "#     end\n",
    "#     if storage4 == nothing\n",
    "#         rho_star_old_i = similar(params)\n",
    "#     end\n",
    "#     M = size(storage_samples, 1)\n",
    "#     # Perform gradient descent update of i-th π*    \n",
    "#     @inbounds for iter in 1:maxiter\n",
    "#         # Rho is unique up to a constant addative term\n",
    "#         rho_star_old_i = log.(params)\n",
    "#         # Sample Z with current π*\n",
    "#         categorical_variational_distribution = Multinomial(1, params)\n",
    "#         # Populate preallocated arrays with samples from variational distribution\n",
    "#         rand!(categorical_variational_distribution, storage_samples)\n",
    "#         # Set gradient of ELBO to 0\n",
    "#         fill!(grad_L, 0)\n",
    "#         # Rao Blackwellized ELBO\n",
    "#         ELBO = 0\n",
    "#         # Calculate the gradient estimate of the m-th sample\n",
    "#         @inbounds for m in 1:M\n",
    "#             z_m = storage_samples[m]\n",
    "#             # Calculate gradient of log(q_1i(Z_i)) w.r.t. π*_i\n",
    "#             grad_log_q .= z_m .- params\n",
    "#             # Calculate log(q_1i(Z_i))\n",
    "#             log_q = dot(z_m, log.(params))\n",
    "#             # Get log likelihood of m-th sample\n",
    "#             log_prob = modelloglikelihood_func(z_m)\n",
    "#             # Update average gradient\n",
    "#             grad_L .= (m - 1)/m .* grad_L + 1/m .* grad_log_q .* (log_prob - log_q)\n",
    "#             # Update ELBO estimator\n",
    "#             ELBO = (m-1)/m * ELBO + 1/m * (log_prob - log_q)\n",
    "#         end\n",
    "#         # Print ELBO, parameter and gradient if verbose\n",
    "#         if verbose\n",
    "#             println(\"ELBO: $ELBO\")\n",
    "#             println(\"π*_$i: $params\")\n",
    "#             println(\"gradient: $grad_L\")\n",
    "#         end\n",
    "#         # Update with one step\n",
    "#         rho_star_old_i .+= step * grad_L\n",
    "#         # Convert logits into probabilities\n",
    "#         params .= exp.(rho_star_old_i) ./ sum(exp.(rho_star_old_i))\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_categorical_variational_distribution (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_categorical_variational_distribution(\n",
    "    model               :: TDCModel;\n",
    "    step                :: T = 1e-2,\n",
    "    tol                 :: T = 1e-6,\n",
    "    maxiter             :: Int = 100000,\n",
    "    verbose             :: Bool = true\n",
    ") where T <: AbstractFloat\n",
    "    obs = model.obs\n",
    "    Y, D, X = obs.Y, obs.D, obs.X\n",
    "    Z_sample, beta_sample, gamma_sample = model.Z_sample, model.beta_sample, model.gamma_sample\n",
    "    pi_star_old = model.pi_star\n",
    "    # Number of students, time points, questions, skills, attribute profiles, groups\n",
    "    N, O, J, K, L, S = size(obs.Y, 1), size(obs.Y, 2), size(obs.Y, 3),  size(obs.Q, 2), size(obs.D[1], 1), size(obs.U[1][1], 1)\n",
    "    M = model.M\n",
    "    # Fully update parameters of each Z_i using noisy gradients before moving to update parameters of next Z_i\n",
    "    if !model.enable_parallel\n",
    "        @inbounds for i in 1:N\n",
    "            # Storage for gradient terms\n",
    "            grad_log_q = model.storage_L2\n",
    "            grad_L = model.storage_L3\n",
    "            # Storage for intermediate term in gradient calculations\n",
    "            D_beta = model.storage_L\n",
    "            rho_star_old_i = view(model.storage_LL3, 1:L)\n",
    "            # Get parameters for variational distribution of skill of i-th student\n",
    "            pi_star_old_i = pi_star_old[i][1]\n",
    "            # Get group number of student i\n",
    "            group_i = obs.group[i]\n",
    "            # Perform gradient descent update of i-th π*    \n",
    "            @inbounds for iter in 1:maxiter\n",
    "                # Rho is unique up to a constant addative term\n",
    "                rho_star_old_i = log.(pi_star_old_i)\n",
    "                # Sample Z with updated π*\n",
    "                sample_Z(model, i, 1)\n",
    "                # Set gradient of ELBO to 0\n",
    "                fill!(grad_L, 0)\n",
    "                # Rao Blackwellized ELBO\n",
    "                ELBO = 0\n",
    "                # Calculate the gradient estimate of the m-th sample\n",
    "                @inbounds for m in 1:M\n",
    "                    z_im = Z_sample[i][1][m]\n",
    "                    # Calculate gradient of log(q_1i(Z_i)) w.r.t. π*_i\n",
    "                    grad_log_q .= z_im .- pi_star_old_i\n",
    "                    # Calculate log(p(Y, Z_(i)))\n",
    "                    log_prob_YZ = 0\n",
    "                    @inbounds for j in 1:J\n",
    "                        mul!(D_beta, D[j], beta_sample[j][m])\n",
    "                        log_prob_YZ += dot(z_im, log.(sigmoid.((2*Y[i, 1, j] - 1) .* D_beta)))\n",
    "                    end\n",
    "                    skill_profile = obs.skill_dict[argmax(z_im)]\n",
    "                    @inbounds for k in 1:K\n",
    "                        log_prob_YZ += log(sigmoid((2*skill_profile[k] - 1) * dot(gamma_sample[k][1][1][group_i][m], obs.X[k][1][i])))\n",
    "                    end\n",
    "                    # Calculate log(q_1i(Z_i))\n",
    "                    log_q = dot(z_im, log.(pi_star_old_i))\n",
    "                    # Update average gradient\n",
    "                    grad_L .= (m - 1)/m .* grad_L + 1/m .* grad_log_q .* (log_prob_YZ - log_q)\n",
    "                    # Update ELBO estimator\n",
    "                    ELBO = (m-1)/m * ELBO + 1/m * (log_prob_YZ - log_q)\n",
    "                end\n",
    "                # Print ELBO, parameter and gradient if verbose\n",
    "                if verbose\n",
    "                    println(\"ELBO: $ELBO\")\n",
    "                    println(\"π*_$i: $pi_star_old_i\")\n",
    "                    println(\"gradient: $grad_L\")\n",
    "                end\n",
    "                # Update with one step\n",
    "                rho_star_old_i .+= step * grad_L\n",
    "                # Convert logits into probabilities\n",
    "                pi_star_old_i .= exp.(rho_star_old_i) ./ sum(exp.(rho_star_old_i))\n",
    "                # Stop condition\n",
    "                if abs2(norm(grad_L)) <= tol\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        Threads.@threads for i in 1:N\n",
    "            # Get thread id\n",
    "            tid = Threads.threadid()\n",
    "            # Storage for gradient terms\n",
    "            grad_log_q = model.storage_L2_par[tid]\n",
    "            grad_L = model.storage_L3_par[tid]\n",
    "            # Storage for intermediate term in gradient calculations\n",
    "            D_beta = model.storage_L_par[tid]\n",
    "            rho_star_old_i = view(model.storage_LL3_par[tid], 1:L)\n",
    "            # Get parameters for variational distribution of skill of i-th student\n",
    "            pi_star_old_i = pi_star_old[i][1]\n",
    "            # Get group number of student i\n",
    "            group_i = obs.group[i]\n",
    "            # Perform gradient descent update of i-th π*    \n",
    "            @inbounds for iter in 1:maxiter\n",
    "                # Rho is unique up to a constant addative term\n",
    "                rho_star_old_i = log.(pi_star_old_i)\n",
    "                # Sample Z with updated π*\n",
    "                sample_Z(model, i, 1)\n",
    "                # Set gradient of ELBO to 0\n",
    "                fill!(grad_L, 0)\n",
    "                # Rao Blackwellized ELBO\n",
    "                ELBO = 0\n",
    "                # Calculate the gradient estimate of the m-th sample\n",
    "                @inbounds for m in 1:M\n",
    "                    z_im = Z_sample[i][1][m]\n",
    "                    # Calculate gradient of log(q_1i(Z_i)) w.r.t. π*_i\n",
    "                    grad_log_q .= z_im .- pi_star_old_i\n",
    "                    # Calculate log(p(Y, Z_(i)))\n",
    "                    log_prob_YZ = 0\n",
    "                    for j in 1:J\n",
    "                        mul!(D_beta, D[j], beta_sample[j][m])\n",
    "                        log_prob_YZ += dot(z_im, log.(sigmoid.((2*Y[i, 1, j] - 1) .* D_beta)))\n",
    "                    end\n",
    "                    skill_profile = obs.skill_dict[argmax(z_im)]\n",
    "                    for k in 1:K\n",
    "                        log_prob_YZ += log(sigmoid((2*skill_profile[k] - 1) * dot(gamma_sample[k][1][1][group_i][m], obs.X[k][1][i])))\n",
    "                    end\n",
    "                    # Calculate log(q_1i(Z_i))\n",
    "                    log_q = dot(z_im, log.(pi_star_old_i))\n",
    "                    # Update average gradient\n",
    "                    grad_L .= (m - 1)/m .* grad_L + 1/m .* grad_log_q .* (log_prob_YZ - log_q)\n",
    "                    # Update ELBO estimator\n",
    "                    ELBO = (m-1)/m * ELBO + 1/m * (log_prob_YZ - log_q)\n",
    "                end\n",
    "                # Print ELBO, parameter and gradient if verbose\n",
    "                if verbose\n",
    "                    println(\"ELBO: $ELBO\")\n",
    "                    println(\"π*_$i: $pi_star_old_i\")\n",
    "                    println(\"gradient: $grad_L\")\n",
    "                end\n",
    "                # Update with one step\n",
    "                rho_star_old_i .+= step * grad_L\n",
    "                # Convert logits into probabilities\n",
    "                pi_star_old_i .= exp.(rho_star_old_i) ./ sum(exp.(rho_star_old_i))\n",
    "                # Stop condition\n",
    "                if abs2(norm(grad_L)) <= tol\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_categorical_variational_distribution2 (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_categorical_variational_distribution2(\n",
    "    model               :: TDCModel,\n",
    "    time                :: Int;\n",
    "    step                :: T = 1e-2,\n",
    "    tol                 :: T = 1e-6,\n",
    "    maxiter             :: Int = 100000,\n",
    "    verbose             :: Bool = true\n",
    ") where T <: AbstractFloat\n",
    "    obs = model.obs\n",
    "    Y, D, X = obs.Y, obs.D, obs.X\n",
    "    Z_sample, beta_sample, gamma_sample = model.Z_sample, model.beta_sample, model.gamma_sample\n",
    "    pi_star_old = model.pi_star\n",
    "    # Number of students, time points, questions, skills, attribute profiles, groups\n",
    "    N, O, J, K, L, S = size(obs.Y, 1), size(obs.Y, 2), size(obs.Y, 3),  size(obs.Q, 2), size(obs.D[1], 1), size(obs.U[1][1], 1)\n",
    "    M = model.M\n",
    "    # Fully update parameters of each Z_i using noisy gradients before moving to update parameters of next Z_i\n",
    "    if !model.enable_parallel\n",
    "        @inbounds for i in 1:N\n",
    "            # Storage for gradient terms\n",
    "            grad_log_q = model.storage_L2\n",
    "            grad_L = model.storage_L3\n",
    "            # Storage for intermediate term in gradient calculations\n",
    "            D_beta = model.storage_L\n",
    "            rho_star_old_i = view(model.storage_LL3, 1:L)\n",
    "            # Get parameters for variational distribution of skill of i-th student\n",
    "            pi_star_old_i = pi_star_old[i][time]\n",
    "            # Get group number of student i\n",
    "            group_i = obs.group[i]\n",
    "            # Perform gradient descent update of i-th π*    \n",
    "            for iter in 1:maxiter\n",
    "                # Rho is unique up to a constant addative term\n",
    "                rho_star_old_i = log.(pi_star_old_i)\n",
    "                # Sample Z with updated π*\n",
    "                sample_Z(model, i, time)\n",
    "                # Set gradient of ELBO to 0\n",
    "                fill!(grad_L, 0)\n",
    "                # Rao Blackwellized ELBO\n",
    "                ELBO = 0\n",
    "                # Calculate the gradient estimate of the m-th sample\n",
    "                for m in 1:M\n",
    "                    z_im = Z_sample[i][time][m]\n",
    "                    # Calculate gradient of log(q_1i(Z_i)) w.r.t. π*_i\n",
    "                    grad_log_q .= z_im .- pi_star_old_i\n",
    "                    # Calculate log(p(Y, Z_(i)))\n",
    "                    log_prob_YZ = 0\n",
    "                    for j in 1:J\n",
    "                        mul!(D_beta, D[j], beta_sample[j][m])\n",
    "                        log_prob_YZ += dot(z_im, log.(sigmoid.((2*Y[i, time, j] - 1) .* D_beta)))\n",
    "                    end\n",
    "                    skill_profile = obs.skill_dict[argmax(z_im)]\n",
    "                    prev_skill_profile = obs.skill_dict[argmax(Z_sample[i][time - 1][m])]\n",
    "                    for k in 1:K\n",
    "                        log_prob_YZ += log(sigmoid((2*skill_profile[k] - 1) * \n",
    "                                    dot(gamma_sample[k][time][prev_skill_profile[k] + 1][group_i][m], obs.X[k][time][i, :])))\n",
    "                    end\n",
    "                    # Calculate log(q_1i(Z_i))\n",
    "                    log_q = dot(z_im, log.(pi_star_old_i))\n",
    "                    # Update average gradient\n",
    "                    grad_L .= (m - 1)/m .* grad_L + 1/m .* grad_log_q .* (log_prob_YZ - log_q)\n",
    "                    # Update ELBO estimator\n",
    "                    ELBO = (m-1)/m * ELBO + 1/m * (log_prob_YZ - log_q)\n",
    "                end\n",
    "                # Print ELBO, parameter and gradient if verbose\n",
    "                if verbose\n",
    "                    println(\"ELBO: $ELBO\")\n",
    "                    println(\"π*_$i: $pi_star_old_i\")\n",
    "                    println(\"gradient: $grad_L\")\n",
    "                end\n",
    "                # Update with one step\n",
    "                rho_star_old_i .+= step * grad_L\n",
    "                # Convert logits into probabilities\n",
    "                pi_star_old_i .= exp.(rho_star_old_i) ./ sum(exp.(rho_star_old_i))\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        Threads.@threads for i in 1:N\n",
    "            # Get thread id\n",
    "            tid = Threads.threadid()\n",
    "            # Storage for gradient terms\n",
    "            grad_log_q = model.storage_L2_par[tid]\n",
    "            grad_L = model.storage_L3_par[tid]\n",
    "            # Storage for intermediate term in gradient calculations\n",
    "            D_beta = model.storage_L_par[tid]\n",
    "            rho_star_old_i = view(model.storage_LL3_par[tid], 1:L)\n",
    "            # Get parameters for variational distribution of skill of i-th student\n",
    "            pi_star_old_i = pi_star_old[i][time]\n",
    "            # Get group number of student i\n",
    "            group_i = obs.group[i]\n",
    "            # Perform gradient descent update of i-th π*    \n",
    "            @inbounds for iter in 1:maxiter\n",
    "                # Rho is unique up to a constant addative term\n",
    "                rho_star_old_i = log.(pi_star_old_i)\n",
    "                # Sample Z with updated π*\n",
    "                sample_Z(model, i, time)\n",
    "                # Set gradient of ELBO to 0\n",
    "                fill!(grad_L, 0)\n",
    "                # Rao Blackwellized ELBO\n",
    "                ELBO = 0\n",
    "                # Calculate the gradient estimate of the m-th sample\n",
    "                for m in 1:M\n",
    "                    z_im = Z_sample[i][time][m]\n",
    "                    # Calculate gradient of log(q_1i(Z_i)) w.r.t. π*_i\n",
    "                    grad_log_q .= z_im .- pi_star_old_i\n",
    "                    # Calculate log(p(Y, Z_(i)))\n",
    "                    log_prob_YZ = 0\n",
    "                    for j in 1:J\n",
    "                        mul!(D_beta, D[j], beta_sample[j][m])\n",
    "                        log_prob_YZ += dot(z_im, log.(sigmoid.((2*Y[i, time, j] - 1) .* D_beta)))\n",
    "                    end\n",
    "                    skill_profile = obs.skill_dict[argmax(z_im)]\n",
    "                    prev_skill_profile = obs.skill_dict[argmax(Z_sample[i][time - 1][m])]\n",
    "                    for k in 1:K\n",
    "                        log_prob_YZ += log(sigmoid((2*skill_profile[k] - 1) * \n",
    "                                    dot(gamma_sample[k][time][prev_skill_profile[k] + 1][group_i][m], obs.X[k][time][i, :])))\n",
    "                    end\n",
    "                    # Calculate log(q_1i(Z_i))\n",
    "                    log_q = dot(z_im, log.(pi_star_old_i))\n",
    "                    # Update average gradient\n",
    "                    grad_L .= (m - 1)/m .* grad_L + 1/m .* grad_log_q .* (log_prob_YZ - log_q)\n",
    "                    # Update ELBO estimator\n",
    "                    ELBO = (m-1)/m * ELBO + 1/m * (log_prob_YZ - log_q)\n",
    "                end\n",
    "                # Print ELBO, parameter and gradient if verbose\n",
    "                if verbose\n",
    "                    println(\"ELBO: $ELBO\")\n",
    "                    println(\"π*_$i: $pi_star_old_i\")\n",
    "                    println(\"gradient: $grad_L\")\n",
    "                end\n",
    "                # Update with one step\n",
    "                rho_star_old_i .+= step * grad_L\n",
    "                # Convert logits into probabilities\n",
    "                pi_star_old_i .= exp.(rho_star_old_i) ./ sum(exp.(rho_star_old_i))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_normal_variational_distribution(\n",
    "    model       :: TDCModel;\n",
    "    init_step   :: T=1e-3,\n",
    "    step_iterator=get_robbins_monroe_iterator(init_step, 20),\n",
    "    # step_iterator_factory=get_robbins_monroe_iterator,\n",
    "    use_iter    :: Bool=false,\n",
    "    tol         :: T=1e-6,\n",
    "    maxiter     :: Int=100000,\n",
    "    verbose     :: Bool=true\n",
    ") where T <: AbstractFloat\n",
    "    obs = model.obs\n",
    "    Y, D = Matrix{T}(obs.Y), Vector{Matrix{T}}(obs.D)\n",
    "    Z_sample = model.Z_sample\n",
    "    mu_star_old, V_star_old = model.mu_star, model.V_star\n",
    "    N, J, L = size(Y, 1), size(Y, 2), size(D[1], 1)\n",
    "    M = model.M\n",
    "    # Sample Z, β, and sigma^2. Only β samples will update as the parameters update\n",
    "    sample_variational_distribution(model, sample_Z=true, sample_sigma2=true)\n",
    "    # Fully update parameters of each β_j using noisy gradients before moving to update parameters of next β_j\n",
    "    if !model.enable_parallel\n",
    "        @inbounds for j in 1:J\n",
    "            mu_star_old_j = mu_star_old[j]\n",
    "            V_star_old_j = V_star_old[j]\n",
    "            # Perform gradient descent update of mu_j and V_j\n",
    "            len_beta = length(beta_sample[j][1])\n",
    "            # Assign storage for gradient terms\n",
    "            # Memory assigned from preallocated storage\n",
    "            # Memory has to be strided (equal stride between memory addresses) to work with BLAS and LAPACK \n",
    "            # (important for vectorized matricies to be strided if we want to use them for linear algebra)\n",
    "            # Matricies are stored column major in Julia, so memory is assigned by column left to right\n",
    "            grad_mu_L = view(model.storage_L, 1:len_beta)\n",
    "            grad_C_L = view(model.storage_LL2, 1:len_beta, 1:len_beta)\n",
    "            vech_grad_C_L = view(grad_C_L, [len_beta * (j - 1) + i for j in 1:len_beta for i in j:len_beta]) # Uses same memory as grad_C_L\n",
    "            grad_mu_log_q = view(model.storage_L2, 1:len_beta)\n",
    "            vec_grad_V_log_q = view(model.storage_LL3, 1:len_beta^2)\n",
    "            grad_V_log_q = reshape(vec_grad_V_log_q, len_beta, len_beta) # Uses same memory as vec_grad_V_log_q\n",
    "            # Assign storage for calculating intermediate terms for gradient\n",
    "            Vinv_star_old_j = view(model.storage_LL, 1:len_beta, 1:len_beta)\n",
    "            beta_minus_mu = view(model.storage_L3, 1:len_beta)\n",
    "            C_star_old_j = view(model.storage_C, 1:len_beta, 1:len_beta)\n",
    "            vech_C_star_old_j = view(C_star_old_j, [len_beta * (j - 1) + i for j in 1:len_beta for i in j:len_beta]) # Uses same memory as C_star_old_j\n",
    "            fill!(C_star_old_j, 0)\n",
    "            storage_kron_prod = view(model.storage_L2L2, 1:len_beta^2, 1:len_beta^2)\n",
    "            storage_len_beta_sqr = view(model.storage_Lsqr, 1:len_beta^2)\n",
    "            storage_len_beta_sqr2 = view(model.storage_Lsqr2, 1:len_beta^2)\n",
    "            storage_gradC = view(model.storage_gradC, 1:Int(len_beta * (len_beta + 1) / 2))\n",
    "            # Generate commutation and duplication matrix\n",
    "            comm_j = view(model.storage_comm, 1:len_beta^2, 1:len_beta^2)\n",
    "            dup_j = view(model.storage_dup, 1:len_beta^2, 1:Int(len_beta * (len_beta + 1) / 2))\n",
    "            get_comm!(comm_j, len_beta)\n",
    "            get_dup!(dup_j, len_beta)\n",
    "            # Assign len_beta by len_beta identity matrix\n",
    "            I_j = view(model.I_LL, 1:len_beta, 1:len_beta)\n",
    "            # # Get step size iterator\n",
    "            # step_iterator = step_iterator_factory(init_step)\n",
    "            # # Initialize variables for tracking previous values\n",
    "            # prev_ELBO = -Inf\n",
    "            # prev_mu = view(model.storage_L4, 1:len_beta)\n",
    "            # prev_V = view(model.storage_LL4, 1:len_beta, 1:len_beta)\n",
    "            # prev_mu .= mu_star_old[j]\n",
    "            # prev_V .= V_star_old[j]\n",
    "            @inbounds for iter in 1:maxiter\n",
    "                # Sample β from variational distribution\n",
    "                sample_variational_distribution(model, sample_β=true, idx_β=j)\n",
    "                fill!(grad_mu_L, 0)\n",
    "                fill!(grad_C_L, 0)\n",
    "                # Copy V* into storage\n",
    "                copy!(Vinv_star_old_j, V_star_old_j)\n",
    "                # Perform cholesky decomposition on V*\n",
    "                # After this step, the lower triangle of Vinv_star_old_j will contain the lower triangular cholesky factor of V*\n",
    "                LAPACK.potrf!('L', Vinv_star_old_j)\n",
    "                # Calculate log|V_j| from diagonal of cholesky decomposition\n",
    "                logdet_V_j = 0\n",
    "                for b in 1:len_beta\n",
    "                    logdet_V_j += 2 * log(Vinv_star_old_j[b, b])\n",
    "                end\n",
    "                # Copy lower triangular cholesky factor into preallocated storage\n",
    "                for k in 1:len_beta\n",
    "                    for l in 1:k\n",
    "                        C_star_old_j[k, l] = Vinv_star_old_j[k, l]\n",
    "                    end\n",
    "                end\n",
    "                # Perform in place matrix inverse on positive definite V* matrix to get V* inverse\n",
    "                LAPACK.potri!('L', Vinv_star_old_j)\n",
    "                LinearAlgebra.copytri!(Vinv_star_old_j, 'L')\n",
    "                ELBO = 0\n",
    "                # Calculate the gradient estimate of the m-th sample\n",
    "                @inbounds for m in 1:M\n",
    "                    beta_jm = beta_sample[j][m]\n",
    "                    fill!(grad_mu_log_q, 0)\n",
    "                    # grad_mu_log_q = Vinv_star * β_jm\n",
    "                    BLAS.gemv!('N', T(1), Vinv_star_old_j, beta_jm, T(1), grad_mu_log_q)\n",
    "                    # grad_mu_log_q = Vinv_star_j * β_jm - Vinv_star_j * mu_star_j\n",
    "                    BLAS.gemv!('N', T(-1), Vinv_star_old_j, mu_star_old_j, T(1), grad_mu_log_q)\n",
    "                    # grad_V_log_q = -1/2(Vinv_star_j - Vinv_star_j * (β_jm - mu_star_j) * (β_jm - mu_star_j)^T * Vinv_star_j)\n",
    "                    copy!(grad_V_log_q, Vinv_star_old_j)\n",
    "                    BLAS.gemm!('N', 'T', T(1 / 2), grad_mu_log_q, grad_mu_log_q, T(-1 / 2), grad_V_log_q)\n",
    "                    # storage_kron_prod = I ⊗ C_j\n",
    "                    collect!(storage_kron_prod, kronecker(I_j, C_star_old_j))\n",
    "                    # storage_len_beta_sqr = (I ⊗ C_j)'vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), storage_kron_prod, vec_grad_V_log_q, T(1), fill!(storage_len_beta_sqr, 0))\n",
    "                    # storage_kron_prod = C_j ⊗ I\n",
    "                    collect!(storage_kron_prod, kronecker(C_star_old_j, I_j))\n",
    "                    # storage_len_beta_sqr2 = (C_j ⊗ I)'vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), storage_kron_prod, vec_grad_V_log_q, T(1), fill!(storage_len_beta_sqr2, 0))\n",
    "                    # storage_len_beta_sqr2 = ((C_j ⊗ I)' + K'(I ⊗ C_j)')vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), comm_j, storage_len_beta_sqr, T(1), storage_len_beta_sqr2)\n",
    "                    # storage_gradC = D'((C_j ⊗ I)' + K'(I ⊗ C_j)')vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), dup_j, storage_len_beta_sqr2, T(1), fill!(storage_gradC, 0))\n",
    "                    # Calculate log(p(Y, β_(j)))\n",
    "                    log_prob_Ybeta = 0\n",
    "                    for i in 1:N\n",
    "                        fill!(model.storage_L3, 0)\n",
    "                        BLAS.gemv!('N', (2 * Y[i, j] - 1), D[j], beta_jm, T(1), model.storage_L3)\n",
    "                        log_prob_Ybeta += dot(Z_sample[i][m], log.(sigmoid.(model.storage_L3)))\n",
    "                    end\n",
    "                    log_prob_Ybeta -= 1 / (2 * sigma2_sample[m]) * dot(beta_jm, beta_jm)\n",
    "                    beta_minus_mu .= beta_jm\n",
    "                    beta_minus_mu .-= mu_star_old_j\n",
    "                    log_q = -len_beta / 2 * log(2 * pi) - 1 / 2 * logdet_V_j - 1 / 2 * dot(beta_minus_mu, grad_mu_log_q)\n",
    "                    # Update average gradient\n",
    "                    grad_mu_L .= (m - 1) / m .* grad_mu_L + 1 / m .* grad_mu_log_q .* (log_prob_Ybeta - log_q)\n",
    "                    vech_grad_C_L .= (m - 1) / m .* vech_grad_C_L + 1 / m .* storage_gradC .* (log_prob_Ybeta - log_q)\n",
    "                    # Update ELBO estimator\n",
    "                    ELBO = (m - 1) / m * ELBO + 1 / m * (log_prob_Ybeta - log_q)\n",
    "                end\n",
    "                # Print ELBO, parameter and gradient if verbose\n",
    "                # if verbose\n",
    "                #     println(\"ELBO: $ELBO\")\n",
    "                #     println(\"mu*_$j: $mu_star_old_j\")\n",
    "                #     println(\"gradient mu: $grad_mu_L\")\n",
    "                #     println(\"C*_$j: $C_star_old_j\")\n",
    "                #     println(\"gradient C: $grad_C_L\")\n",
    "                # end\n",
    "                # Stop condition TODO: update to more appropriate stop condition\n",
    "                # if abs2(norm(vech_C_star_old_j)) > 1e6\n",
    "                #     break\n",
    "                # end\n",
    "                # # If ELBO decreases, go to previous step\n",
    "                # if ELBO < prev_ELBO\n",
    "                #     mu_star_old_j .= prev_mu\n",
    "                #     V_star_old_j .= prev_V\n",
    "                # else\n",
    "                #     # Save current values\n",
    "                #     prev_mu .= mu_star_old_j\n",
    "                #     prev_V .= V_star_old_j\n",
    "                #     prev_ELBO = ELBO\n",
    "                #     # Update mu and C with one step\n",
    "                #     mu_star_old_j .+= step .* grad_mu_L\n",
    "                #     vech_C_star_old_j .+= step .* vech_grad_C_L\n",
    "                #     # Set V_star_old_j = C * C'\n",
    "                #     BLAS.gemm!('N', 'T', T(1), C_star_old_j, C_star_old_j, T(1), fill!(V_star_old_j, 0))\n",
    "                # end\n",
    "                # Update mu and C with one step\n",
    "                step = init_step\n",
    "                if use_iter\n",
    "                    step = step_iterator()\n",
    "                    println(\"Question $j: $step\")\n",
    "                end\n",
    "                mu_star_old_j .+= step .* grad_mu_L ./ norm(grad_mu_L)\n",
    "                vech_C_star_old_j .+= step .* vech_grad_C_L ./ norm(vech_grad_C_L)\n",
    "                # Set V_star_old_j = C * C'\n",
    "                BLAS.gemm!('N', 'T', T(1), C_star_old_j, C_star_old_j, T(1), fill!(V_star_old_j, 0))\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        Threads.@threads for j in 1:J\n",
    "            # Get thread id\n",
    "            tid = Threads.threadid()\n",
    "            # Perform gradient descent update of mu_j and V_j\n",
    "            len_beta = length(beta_sample[j][1])\n",
    "            # Assign storage for gradient terms\n",
    "            # Memory assigned from preallocated storage\n",
    "            # Memory has to be strided (equal stride between memory addresses) to work with BLAS and LAPACK \n",
    "            # (important for vectorized matricies to be strided if we want to use them for linear algebra)\n",
    "            # Matricies are stored column major in Julia, so memory is assigned by column left to right\n",
    "            grad_mu_L = view(model.storage_L_par[tid], 1:len_beta)\n",
    "            grad_C_L = view(model.storage_LL2_par[tid], 1:len_beta, 1:len_beta)\n",
    "            vech_grad_C_L = view(grad_C_L, [len_beta * (j - 1) + i for j in 1:len_beta for i in j:len_beta]) # Uses same memory as grad_C_L\n",
    "            grad_mu_log_q = view(model.storage_L2_par[tid], 1:len_beta)\n",
    "            vec_grad_V_log_q = view(model.storage_LL3_par[tid], 1:len_beta^2)\n",
    "            grad_V_log_q = reshape(vec_grad_V_log_q, len_beta, len_beta) # Uses same memory as vec_grad_V_log_q\n",
    "            # Assign storage for calculating intermediate terms for gradient\n",
    "            Vinv_star_old_j = view(model.storage_LL_par[tid], 1:len_beta, 1:len_beta)\n",
    "            beta_minus_mu = view(model.storage_L3_par[tid], 1:len_beta)\n",
    "            C_star_old_j = view(model.storage_C_par[tid], 1:len_beta, 1:len_beta)\n",
    "            vech_C_star_old_j = view(C_star_old_j, [len_beta * (j - 1) + i for j in 1:len_beta for i in j:len_beta]) # Uses same memory as C_star_old_j\n",
    "            fill!(C_star_old_j, 0)\n",
    "            storage_kron_prod = view(model.storage_L2L2_par[tid], 1:len_beta^2, 1:len_beta^2)\n",
    "            storage_len_beta_sqr = view(model.storage_Lsqr_par[tid], 1:len_beta^2)\n",
    "            storage_len_beta_sqr2 = view(model.storage_Lsqr2_par[tid], 1:len_beta^2)\n",
    "            storage_gradC = view(model.storage_gradC_par[tid], 1:Int(len_beta * (len_beta + 1) / 2))\n",
    "            # Generate commutation and duplication matrix\n",
    "            comm_j = view(model.storage_comm_par[tid], 1:len_beta^2, 1:len_beta^2)\n",
    "            dup_j = view(model.storage_dup_par[tid], 1:len_beta^2, 1:Int(len_beta * (len_beta + 1) / 2))\n",
    "            get_comm!(comm_j, len_beta)\n",
    "            get_dup!(dup_j, len_beta)\n",
    "            # Assign len_beta by len_beta identity matrix\n",
    "            I_j = view(model.I_LL, 1:len_beta, 1:len_beta)\n",
    "            # # Get step iterator\n",
    "            # step_iterator = step_iterator_factory(init_step)\n",
    "            # # Initialize variables for tracking previous values\n",
    "            # prev_ELBO = -Inf\n",
    "            # prev_mu = view(model.storage_L4, 1:len_beta)\n",
    "            # prev_V = view(model.storage_LL4, 1:len_beta, 1:len_beta)\n",
    "            # prev_mu .= mu_star_old[j]\n",
    "            # prev_V .= V_star_old[j]\n",
    "            @inbounds for iter in 1:maxiter\n",
    "                # Sample β from variational distribution\n",
    "                sample_variational_distribution(model, sample_β=true, idx_β=j)\n",
    "                fill!(grad_mu_L, 0)\n",
    "                fill!(grad_C_L, 0)\n",
    "                mu_star_old_j = mu_star_old[j]\n",
    "                V_star_old_j = V_star_old[j]\n",
    "                # Copy V* into storage\n",
    "                copy!(Vinv_star_old_j, V_star_old_j)\n",
    "                # Perform cholesky decomposition on V*\n",
    "                # After this step, the lower triangle of Vinv_star_old_j will contain the lower triangular cholesky factor of V*\n",
    "                LAPACK.potrf!('L', Vinv_star_old_j)\n",
    "                # Calculate log|V_j| from diagonal of cholesky decomposition\n",
    "                logdet_V_j = 0\n",
    "                for b in 1:len_beta\n",
    "                    logdet_V_j += 2 * log(Vinv_star_old_j[b, b])\n",
    "                end\n",
    "                # Copy lower triangular cholesky factor into preallocated storage\n",
    "                for k in 1:len_beta\n",
    "                    for l in 1:k\n",
    "                        C_star_old_j[k, l] = Vinv_star_old_j[k, l]\n",
    "                    end\n",
    "                end\n",
    "                # Perform in place matrix inverse on positive definite V* matrix to get V* inverse\n",
    "                LAPACK.potri!('L', Vinv_star_old_j)\n",
    "                LinearAlgebra.copytri!(Vinv_star_old_j, 'L')\n",
    "                ELBO = 0\n",
    "                # Calculate the gradient estimate of the m-th sample\n",
    "                @inbounds for m in 1:M\n",
    "                    beta_jm = beta_sample[j][m]\n",
    "                    fill!(grad_mu_log_q, 0)\n",
    "                    # grad_mu_log_q = Vinv_star * β_jm\n",
    "                    BLAS.gemv!('N', T(1), Vinv_star_old_j, beta_jm, T(1), grad_mu_log_q)\n",
    "                    # grad_mu_log_q = Vinv_star_j * β_jm - Vinv_star_j * mu_star_j\n",
    "                    BLAS.gemv!('N', T(-1), Vinv_star_old_j, mu_star_old_j, T(1), grad_mu_log_q)\n",
    "                    # grad_V_log_q = -1/2(Vinv_star_j - Vinv_star_j * (β_jm - mu_star_j) * (β_jm - mu_star_j)^T * Vinv_star_j)\n",
    "                    copy!(grad_V_log_q, Vinv_star_old_j)\n",
    "                    BLAS.gemm!('N', 'T', T(1 / 2), grad_mu_log_q, grad_mu_log_q, T(-1 / 2), grad_V_log_q)\n",
    "                    # storage_kron_prod = I ⊗ C_j\n",
    "                    collect!(storage_kron_prod, kronecker(I_j, C_star_old_j))\n",
    "                    # storage_len_beta_sqr = (I ⊗ C_j)'vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), storage_kron_prod, vec_grad_V_log_q, T(1), fill!(storage_len_beta_sqr, 0))\n",
    "                    # storage_kron_prod = C_j ⊗ I\n",
    "                    collect!(storage_kron_prod, kronecker(C_star_old_j, I_j))\n",
    "                    # storage_len_beta_sqr2 = (C_j ⊗ I)'vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), storage_kron_prod, vec_grad_V_log_q, T(1), fill!(storage_len_beta_sqr2, 0))\n",
    "                    # storage_len_beta_sqr2 = ((C_j ⊗ I)' + K'(I ⊗ C_j)')vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), comm_j, storage_len_beta_sqr, T(1), storage_len_beta_sqr2)\n",
    "                    # storage_gradC = D'((C_j ⊗ I)' + K'(I ⊗ C_j)')vec(grad_V_log_q)\n",
    "                    BLAS.gemv!('T', T(1), dup_j, storage_len_beta_sqr2, T(1), fill!(storage_gradC, 0))\n",
    "                    # Calculate log(p(Y, β_(j)))\n",
    "                    log_prob_Ybeta = 0\n",
    "                    for i in 1:N\n",
    "                        fill!(model.storage_L3_par[tid], 0)\n",
    "                        BLAS.gemv!('N', (2 * Y[i, j] - 1), D[j], beta_jm, T(1), model.storage_L3_par[tid])\n",
    "                        log_prob_Ybeta += dot(Z_sample[i][m], log.(sigmoid.(model.storage_L3_par[tid])))\n",
    "                    end\n",
    "                    log_prob_Ybeta -= 1 / (2 * sigma2_sample[m]) * dot(beta_jm, beta_jm)\n",
    "                    beta_minus_mu .= beta_jm\n",
    "                    beta_minus_mu .-= mu_star_old_j\n",
    "                    log_q = -len_beta / 2 * log(2 * pi) - 1 / 2 * logdet_V_j - 1 / 2 * dot(beta_minus_mu, grad_mu_log_q)\n",
    "                    # Update average gradient\n",
    "                    grad_mu_L .= (m - 1) / m .* grad_mu_L + 1 / m .* grad_mu_log_q .* (log_prob_Ybeta - log_q)\n",
    "                    vech_grad_C_L .= (m - 1) / m .* vech_grad_C_L + 1 / m .* storage_gradC .* (log_prob_Ybeta - log_q)\n",
    "                    # Update ELBO estimator\n",
    "                    ELBO = (m - 1) / m * ELBO + 1 / m * (log_prob_Ybeta - log_q)\n",
    "                end\n",
    "                step = init_step\n",
    "                if use_iter\n",
    "                    step = step_iterator()\n",
    "                end\n",
    "                mu_star_old_j .+= step .* grad_mu_L ./ norm(grad_mu_L)\n",
    "                vech_C_star_old_j .+= step .* vech_grad_C_L ./ norm(vech_grad_C_L)\n",
    "                # Set V_star_old_j = C * C'\n",
    "                BLAS.gemm!('N', 'T', T(1), C_star_old_j, C_star_old_j, T(1), fill!(V_star_old_j, 0))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RCall\n",
    "\n",
    "R\"\"\"\n",
    "load(\"TDCM_multilevel_data.RData\")\n",
    "\"\"\"\n",
    "TDCM_data = @rget data\n",
    "Y = Array{Int, 3}(TDCM_data[:Y])\n",
    "Q = convert(Matrix{Int64}, TDCM_data[:Q_matrix])\n",
    "U = Vector{Vector{Matrix{Float64}}}(TDCM_data[:X_group])\n",
    "for skill in TDCM_data[:X_ind]\n",
    "    for time in 1:length(skill)\n",
    "        if skill[time] isa Vector{<: Number}\n",
    "            skill[time] = reshape(skill[time], :, 1)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "X = Vector{Vector{Matrix{Float64}}}(TDCM_data[:X_ind])\n",
    "group = Vector{Int64}(TDCM_data[:group])\n",
    "obs = TDCMObs(Y, Q, U, X, group)\n",
    "\n",
    "# R\"\"\"\n",
    "# load(\"TDCM_multilevel_J50_data.RData\")\n",
    "# \"\"\"\n",
    "# TDCM_J50_data = @rget data_large_questions\n",
    "# Y = Array{Int, 3}(TDCM_J50_data[:Y])\n",
    "# Q = convert(Matrix{Int64}, TDCM_J50_data[:Q_matrix])\n",
    "# U = Vector{Vector{Matrix{Float64}}}(TDCM_J50_data[:X_group])\n",
    "# for skill in TDCM_J50_data[:X_ind]\n",
    "#     for time in 1:length(skill)\n",
    "#         if skill[time] isa Vector{<: Number}\n",
    "#             skill[time] = reshape(skill[time], :, 1)\n",
    "#         end\n",
    "#     end\n",
    "# end\n",
    "# X = Vector{Vector{Matrix{Float64}}}(TDCM_J50_data[:X_ind])\n",
    "# group = Vector{Int64}(TDCM_J50_data[:group])\n",
    "# obs_J50 = TDCMObs(Y, Q, U, X, group)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDCModel constructed for computation on 12 threads\n"
     ]
    }
   ],
   "source": [
    "N, O, J, K, L, S = size(obs.Y, 1), size(obs.Y, 2), size(obs.Y, 3),  size(obs.Q, 2), size(obs.D[1], 1), size(obs.U[1][1], 1)\n",
    "\n",
    "mu_beta_prior = Vector{Vector{Float64}}(undef, J)\n",
    "V_beta_prior = Vector{Matrix{Float64}}(undef, J)\n",
    "for j in 1:J\n",
    "    num_features = size(obs.D[j], 2)\n",
    "    mu_beta_prior[j] = zeros(num_features)\n",
    "    V_beta_prior[j] = Matrix(1.0I, num_features, num_features)\n",
    "end\n",
    "\n",
    "mu_omega_prior = Vector{Vector{Vector{Vector{Vector{Float64}}}}}(undef, K)\n",
    "V_omega_prior = Vector{Vector{Vector{Vector{Matrix{Float64}}}}}(undef, K)\n",
    "a_tau_prior = Vector{Vector{Vector{Vector{Float64}}}}(undef, K)\n",
    "b_tau_prior = Vector{Vector{Vector{Vector{Float64}}}}(undef, K)\n",
    "\n",
    "for k in 1:K\n",
    "    mu_omega_prior[k] = Vector{Vector{Vector{Vector{Float64}}}}(undef, O)\n",
    "    V_omega_prior[k] = Vector{Vector{Vector{Matrix{Float64}}}}(undef, O)\n",
    "    a_tau_prior[k] = Vector{Vector{Vector{Float64}}}(undef, O)\n",
    "    b_tau_prior[k] = Vector{Vector{Vector{Float64}}}(undef, O)\n",
    "    for t in 1:O\n",
    "        num_features_gamma = size(obs.X[k][t], 2)\n",
    "        num_features_omega = size(obs.U[k][t], 2)\n",
    "        if t == 1\n",
    "            mu_omega_prior[k][t] = Vector{Vector{Vector{Float64}}}(undef, 1)\n",
    "            mu_omega_prior[k][t][1] = Vector{Vector{Float64}}(undef, 1)\n",
    "            mu_omega_prior[k][t][1][1] = zeros(num_features_omega)\n",
    "\n",
    "            V_omega_prior[k][t] = Vector{Vector{Matrix{Float64}}}(undef, 1)\n",
    "            V_omega_prior[k][t][1] = Vector{Matrix{Float64}}(undef, 1)\n",
    "            V_omega_prior[k][t][1][1] = Matrix{Float64}(1.0I, num_features_omega, num_features_omega)\n",
    "\n",
    "            a_tau_prior[k][t] = Vector{Vector{Float64}}(undef, 1)\n",
    "            a_tau_prior[k][t][1] = ones(1)\n",
    "\n",
    "            b_tau_prior[k][t] = Vector{Vector{Float64}}(undef, 1)\n",
    "            b_tau_prior[k][t][1] = ones(1)\n",
    "        else\n",
    "            mu_omega_prior[k][t] = Vector{Vector{Vector{Float64}}}(undef, 2)\n",
    "            V_omega_prior[k][t] = Vector{Vector{Matrix{Float64}}}(undef, 2)\n",
    "            a_tau_prior[k][t] = Vector{Vector{Float64}}(undef, 2)\n",
    "            b_tau_prior[k][t] = Vector{Vector{Float64}}(undef, 2)\n",
    "            for z in 1:2\n",
    "                mu_omega_prior[k][t][z] = Vector{Vector{Float64}}(undef, num_features_gamma)\n",
    "                V_omega_prior[k][t][z] = Vector{Matrix{Float64}}(undef, num_features_gamma)\n",
    "                a_tau_prior[k][t][z] = ones(num_features_gamma) .* 3\n",
    "                b_tau_prior[k][t][z] = ones(num_features_gamma) .* 3\n",
    "                for m in 1:num_features_gamma\n",
    "                    mu_omega_prior[k][t][z][m] = zeros(num_features_omega)\n",
    "                    V_omega_prior[k][t][z][m] = Matrix{Float64}(1.0I, num_features_omega, num_features_omega)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "M = 100\n",
    "model = TDCModel(obs, mu_beta_prior, V_beta_prior, mu_omega_prior, V_omega_prior, a_tau_prior, b_tau_prior, M, enable_parallel=true)\n",
    "\n",
    "# J50 = size(obs_J50.Y, 3)\n",
    "# mu_beta_prior = Vector{Vector{Float64}}(undef, J50)\n",
    "# V_beta_prior = Vector{Matrix{Float64}}(undef, J50)\n",
    "# for j in 1:J50\n",
    "#     num_features = size(obs_J50.D[j], 2)\n",
    "#     mu_beta_prior[j] = zeros(num_features)\n",
    "#     V_beta_prior[j] = Matrix(1.0I, num_features, num_features)\n",
    "# end\n",
    "# model_J50 = TDCModel(obs_J50, mu_beta_prior, V_beta_prior, mu_omega_prior, V_omega_prior, a_tau_prior, b_tau_prior, M, enable_parallel=false)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @resumable pi_t1_loglike = function(Z :: Vector{Int}, Y, :: Vector{Int}, D_beta :: Vector{AbstractFloat})\n",
    "#     for m in 1:model.M\n",
    "#         J = size(Y)\n",
    "#         log_prob_YZ = 0\n",
    "#         for j in 1:J\n",
    "#             mul!(D_beta, D[j], beta_sample[j][m])\n",
    "#             log_prob_YZ += dot(z_im, log.(sigmoid.((2*Y[i, 1, j] - 1) .* D_beta)))\n",
    "#         end\n",
    "#         skill_profile = obs.skill_dict[argmax(z_im)]\n",
    "#         for k in 1:K\n",
    "#             log_prob_YZ += log(sigmoid((2*skill_profile[k] - 1) * dot(gamma_sample[k][1][1][group_i][m], obs.X[k][1][i])))\n",
    "#         end\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimating attribute profiles from uniform initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix true values of gamma and betas\n",
    "for k in 1:K\n",
    "    for s in S\n",
    "        model.mu_gamma_star[k][1][1][s] .= data[:gamma][k][1][s]\n",
    "        model.V_gamma_star[k][1][1][s] = model.V_gamma_star[k][1][1][s] ./ 100\n",
    "    end\n",
    "end\n",
    "for j in 1:J\n",
    "    model.mu_beta_star[j] = data[:beta][j]\n",
    "    model.V_beta_star[j] = model.V_beta_star[j] ./ 100\n",
    "end\n",
    "\n",
    "# Sample beta and gammas\n",
    "sample_β(model)\n",
    "for k in 1:K\n",
    "    for t in 1:O\n",
    "        if t == 1\n",
    "            for s in 1:S\n",
    "                sample_γ(model, s, t, k, 0)\n",
    "            end\n",
    "        else\n",
    "            for z in 0:1\n",
    "                for s in 1:S\n",
    "                    sample_γ(model, s, t, k, z)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_categorical_variational_distribution(model, maxiter=10, verbose=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_categorical_variational_distribution2(model, 2, maxiter=10, verbose=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_profiles = Dict(1=>[0, 0], \n",
    "                 2=>[0, 1],\n",
    "                 3=>[1, 0],\n",
    "                 4=>[1, 1])\n",
    "\n",
    "skill_numbers = Dict([0, 0]=>1, \n",
    "                 [0, 1]=>2,\n",
    "                 [1, 0]=>3,\n",
    "                 [1, 1]=>4)\n",
    "\n",
    "preds = []\n",
    "accuracy = 0\n",
    "attribute_accs = zeros(L)\n",
    "group_accs = zeros(S)\n",
    "\n",
    "attribute_counts = zeros(L)\n",
    "group_counts = zeros(S)\n",
    "\n",
    "for i in 1:5000\n",
    "    pred = skill_profiles[argmax(model.pi_star[i][1])]\n",
    "    actual = Vector{Int}(data[:profiles][i, :, 1])\n",
    "    group_i = obs.group[i]\n",
    "    profile_i = skill_numbers[actual]\n",
    "    push!(preds, pred)\n",
    "    correct = all(pred .== actual)\n",
    "    accuracy += correct\n",
    "    attribute_accs[profile_i] += correct\n",
    "    group_accs[group_i] += correct\n",
    "    attribute_counts[profile_i] += 1\n",
    "    group_counts[group_i] += 1\n",
    "    # if !correct\n",
    "    #     print(Vector{Int}(data[:profiles][i, 1, :]))\n",
    "    #     println(model.pi_star[i][1])\n",
    "    # end\n",
    "end\n",
    "\n",
    "accuracy = accuracy/N\n",
    "attribute_accs .= attribute_accs ./ attribute_counts\n",
    "group_accs .= group_accs ./ group_counts\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.9962143273150844\n",
       " 1.0\n",
       " 0.9985815602836879\n",
       " 1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "accuracy = 0\n",
    "attribute_accs = zeros(L)\n",
    "group_accs = zeros(S)\n",
    "\n",
    "attribute_counts = zeros(L)\n",
    "group_counts = zeros(S)\n",
    "\n",
    "for i in 1:5000\n",
    "    pred = skill_profiles[argmax(model.pi_star[i][2])]\n",
    "    actual = Vector{Int}(data[:profiles][i, :, 2])\n",
    "    group_i = obs.group[i]\n",
    "    profile_i = skill_numbers[actual]\n",
    "    push!(preds, pred)\n",
    "    correct = all(pred .== actual)\n",
    "    accuracy += correct\n",
    "    attribute_accs[profile_i] += correct\n",
    "    group_accs[group_i] += correct\n",
    "    attribute_counts[profile_i] += 1\n",
    "    group_counts[group_i] += 1\n",
    "    # if !correct\n",
    "    #     print(Vector{Int}(data[:profiles][i, 1, :]))\n",
    "    #     println(model.pi_star[i][1])\n",
    "    # end\n",
    "end\n",
    "\n",
    "accuracy = accuracy/N\n",
    "attribute_accs .= attribute_accs ./ attribute_counts\n",
    "group_accs .= group_accs ./ group_counts\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.996538081107814\n",
       " 0.9981818181818182\n",
       " 0.9951298701298701\n",
       " 0.9988962472406181"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 12 Threads 1.10.2",
   "language": "julia",
   "name": "julia-12-threads-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
